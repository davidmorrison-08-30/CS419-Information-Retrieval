{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PHÂN TÍCH NGỮ NGHĨA TƯỜNG MINH (Explicit Semantic Analysis)**\n",
        "\n",
        "---\n",
        "\n",
        "Phân tích ngữ nghĩa tường minh có đặc điểm sau:\n",
        "- Tài liệu và truy vấn được biểu diễn theo vector đa chiều.\n",
        "- Mỗi chiều ngữ nghĩa là một lớp văn bản, được xác định bằng một tập tài liệu văn bản được tuyển chọn trước.\n",
        "- Giá trị mỗi chiều sẽ là giá trị của hàm phân lớp với tập các lớp đã được xác định.\n",
        "\n",
        "**I) XÁC ĐỊNH TẬP CÁC LỚP**\n",
        "\n",
        "***1) SỬ DỤNG NGỮ LIỆU PHÂN LỚP SẴN CÓ***\n",
        "- Số chiều có thể ít.\n",
        "- Một số ngữ liệu phân lớp văn bản của Kaggle: https://www.kaggle.com/datasets/guiyihan/text-classification-20?resource=download\n",
        "\n",
        "***2) ÁP DỤNG PHƯƠNG PHÁP GOM CỤM***\n",
        "- Số chiều N tùy chọn.\n",
        "- Chỉ cần một tập ngữ liệu chưa gán nhãn.\n",
        "- Vector hóa các file trong tập ngữ liệu.\n",
        "- Xác định số phân lớp N chính là số chiều cần biểu diễn bằng phương pháp KMeans\n"
      ],
      "metadata": {
        "id": "dgHdBdjgsUXL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JixG02boOY7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5659701-189b-47e0-ba88-831ecb58b8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "stoplist = stopwords.words(\"english\")\n",
        "\n",
        "def getTerms(content):\n",
        "  \n",
        "  sents = sent_tokenize(content)\n",
        "  words = []\n",
        "  for sent in sents:\n",
        "    words.extend(word_tokenize(sent))\n",
        "\n",
        "  filtered = []\n",
        "  for w in words:\n",
        "    if w in stoplist:\n",
        "      continue\n",
        "    filtered.append(w)\n",
        "\n",
        "  tokens = []\n",
        "  for w in filtered:\n",
        "    if w.isalnum():\n",
        "      tokens.append(w)\n",
        "\n",
        "  ps = PorterStemmer()\n",
        "  terms = []\n",
        "  for w in tokens:\n",
        "    terms.append(ps.stem(w))\n",
        "  return terms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ví dụ**\n",
        "\n",
        "Cho tập văn bản D={d1, d2, d3, d4, d5}. Hãy xác định 3 lớp văn bản từ D.\n",
        "\n",
        "d1 = Romeo and Juliet\n",
        "\n",
        "d2 = Juliet: Oh happy dagger\n",
        "\n",
        "d3 = Romeo died by dagger\n",
        "\n",
        "d4 = \"live free or die\" is from New-Hampshire\n",
        "\n",
        "d5 = He is from New-Hampshire\n"
      ],
      "metadata": {
        "id": "i0-cJHtZ89Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfDTLhMRWY3l",
        "outputId": "2258ede9-c3c7-4482-eb59-30347b38ba1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D = ['Romeo and Juliet', \"Juliet: Oh happy dagger\", \"Romeo died by dagger\", '\"live free or die\" is from New-Hampshire', \"He is from New-Hampshire\"]\n"
      ],
      "metadata": {
        "id": "ynS8MoJfAHAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector hóa các tài liệu"
      ],
      "metadata": {
        "id": "6Jry8fafIfPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "dv = DictVectorizer(sparse=False)\n",
        "\n",
        "DD = []\n",
        "for d in D:\n",
        "  DD.append({\"content\":getTerms(d)})\n",
        "print(DD)\n",
        "vectors = dv.fit_transform(DD)\n",
        "print(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j_WDNYBA768",
        "outputId": "beb3b6eb-a8a5-4624-b8a2-7f1dc25ebdc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': ['romeo', 'juliet']}, {'content': ['juliet', 'oh', 'happi', 'dagger']}, {'content': ['romeo', 'die', 'dagger']}, {'content': ['live', 'free', 'die']}, {'content': ['he']}]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            " [1. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thực hiện gom cụm"
      ],
      "metadata": {
        "id": "oWvQKL8GIkBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(vectors)\n",
        "print(kmeans.labels_)\n",
        "#kmeans.predict([[0, 0], [12, 3]])\n",
        "print(kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emNK6KHrIn4o",
        "outputId": "bfb7c9cf-070a-496c-ed1f-2bbcbd74b59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 2 0]\n",
            "[[0.33333333 0.33333333 0.         0.         0.33333333 0.33333333\n",
            "  0.         0.         0.66666667]\n",
            " [1.         0.         0.         1.         0.         1.\n",
            "  0.         1.         0.        ]\n",
            " [0.         1.         1.         0.         0.         0.\n",
            "  1.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II) HUẤN LUYỆN MÔ HÌNH PHÂN LỚP**\n",
        "\n",
        "- Vector hóa các file trong tập ngữ liệu phân lớp.\n",
        "\n",
        "- Xây dựng mô hình phân lớp đa lớp cho các vector này. Các mô hình có thể là SVM, Naive Bayes, KNN, ...\n",
        "\n",
        "- Dự đoán giá trị phân lớp cho từng tài liệu và dùng kết quả phân lớp này như một vector biểu diễn của tài liệu đó.\n",
        "\n"
      ],
      "metadata": {
        "id": "0rd4eXjbOfCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(C=2, kernel = \"rbf\",tol=0.5, probability=True)\n",
        "classifier.fit(vectors,kmeans.labels_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVJ0a26pUol3",
        "outputId": "f1076dcf-4ad1-4463-eb09-684c772580ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2, probability=True, tol=0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III) TÍNH VECTOR TÀI LIỆU VÀ TRUY VẤN**\n",
        "\n",
        "- Xem truy vấn như một tài liệu, vector hóa tài liệu như trên.\n",
        "\n",
        "- Tính xác suất của vector tài liệu hoặc truy vấn được phân vào các lớp.\n",
        "\n",
        "- Các xác suất này được xem như là giá trị các chiều của vector tài liệu hoặc truy vấn.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aWPltVvYmBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [0,0,1,0,0,0,0,0,1]\n",
        "print(classifier.predict_proba([X]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRwnus95SVm6",
        "outputId": "b4f1b146-6c03-47fe-95ae-cbb3ac8d58ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.45324346 0.33084007 0.21591648]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BÀI TẬP**\n",
        "\n",
        "Áp dụng phương pháp phân tích ngữ nghĩa tường minh cho ngữ liệu Cranfield và đánh giá kết quả truy vấn theo độ đo MAP 11 điểm nội suy."
      ],
      "metadata": {
        "id": "Qn0WADYVSjWu"
      }
    }
  ]
}